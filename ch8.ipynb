{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 命令式和符号式混合编程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def fancy_func(a, b, c, d):\n",
    "    e = add(a, b)\n",
    "    f = add(c, d)\n",
    "    g = add(e, f)\n",
    "    return g\n",
    "\n",
    "fancy_func(1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch使用的是命令式编程，故下面略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异步计算\n",
    "\n",
    "默认情况下，PyTorch中的 GPU 操作是异步的。当调用一个使用 GPU 的函数时，这些操作会在特定的设备上排队但不一定会在稍后立即执行。这就使我们可以并行更多的计算，包括 CPU 或其他 GPU 上的操作。 \n",
    "\n",
    "一般情况下，异步计算的效果对调用者是不可见的，因为\n",
    "\n",
    "1. 每个设备按照它们排队的顺序执行操作\n",
    "\n",
    "2. CPU 和 GPU 之间或两个 GPU 之间复制数据时，PyTorch会自动执行必要的同步操作。因此，计算将按每个操作同步执行的方式进行。 可以通过设置环境变量CUDA_LAUNCH_BLOCKING = 1来强制进行同步计算。当 GPU 产生error时，这可能非常有用。（异步执行时，只有在实际执行操作之后才会报告此类错误，因此堆栈跟踪不会显示请求的位置。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动并行计算\n",
    "\n",
    "GPU操作默认是异步的，当调用一个使用GPU的函数时，这些操作会在特定的设备上排队，但不一定会在稍后执行。这允许并行更多的计算，包括CPU或其他GPU上的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "assert torch.cuda.device_count() >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Benchmark():\n",
    "    def __init__(self, prefix=None):\n",
    "        self.prefix = prefix + ' ' if prefix else ''\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        \n",
    "    def __exit__(self, *args):\n",
    "        print('%stime: %.4f sec' % (self.prefix, time.time() - self.start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(x):\n",
    "    for _ in range(20000):\n",
    "        y = torch.mm(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu1 = torch.rand(size=(100, 100), device='cuda:0')\n",
    "x_gpu2 = torch.rand(size=(100, 100), device='cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on GPU1 time: 1.2626 sec\n",
      "Run on GPU2 time: 1.1179 sec\n"
     ]
    }
   ],
   "source": [
    "with Benchmark('Run on GPU1'):\n",
    "    run(x_gpu1)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "with Benchmark('Run on GPU2'):\n",
    "    run(x_gpu2)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on both GPU1 and GPU2 in parallel time: 1.8384 sec\n"
     ]
    }
   ],
   "source": [
    "with Benchmark('Run on both GPU1 and GPU2 in parallel'):\n",
    "    run(x_gpu1)\n",
    "    run(x_gpu2)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多GPU计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct  8 08:13:06 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.74       Driver Version: 418.74       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   59C    P0    77W / 149W |   7695MiB / 11441MiB |     61%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           Off  | 00000000:05:00.0 Off |                    0 |\n",
      "| N/A   47C    P0    80W / 149W |   7220MiB / 11441MiB |     34%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla K80           Off  | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   61C    P0    68W / 149W |   6034MiB / 11441MiB |     41%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla K80           Off  | 00000000:09:00.0 Off |                    0 |\n",
      "| N/A   47C    P0    80W / 149W |   6908MiB / 11441MiB |      8%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla K80           Off  | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   59C    P0    69W / 149W |  11218MiB / 11441MiB |     26%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla K80           Off  | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    78W / 149W |   6830MiB / 11441MiB |     39%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla K80           Off  | 00000000:88:00.0 Off |                    0 |\n",
      "| N/A   61C    P0    66W / 149W |   6814MiB / 11441MiB |     29%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla K80           Off  | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    77W / 149W |   6834MiB / 11441MiB |     21%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=1, bias=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "net = torch.nn.Linear(10, 1).cuda()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torch.nn.DataParallel(net)\n",
    "net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
